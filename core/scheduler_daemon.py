#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–î–µ–º–æ–Ω-–ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è HH-–±–æ—Ç–∞ v4
–†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π 2.7 (–î–∏—Å–ø–µ—Ç—á–µ—Ä –∑–∞–¥–∞—á) –∏ 3.2 (–û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å)

// Chg_SCHEDULER_DAEMON_2009: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –¥–∏—Å–ø–µ—Ç—á–µ—Ä –∑–∞–¥–∞—á —Å –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–æ–º
"""

import asyncio
import logging
import time
import json
import signal
import sys
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, field
from pathlib import Path
from enum import Enum
import threading

# –ò–º–ø–æ—Ä—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã
from .task_dispatcher import TaskDispatcher
from .task_database import TaskDatabase
from plugins.fetcher_v4 import VacancyFetcher, estimate_total_pages
from logging.handlers import RotatingFileHandler
from core.config_manager import get_config_manager


class TaskType(Enum):
    """–¢–∏–ø—ã –∑–∞–¥–∞—á –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞"""
    FETCH_VACANCIES = "fetch_vacancies"
    FETCH_EMPLOYERS = "fetch_employers" 
    CLEANUP_DATA = "cleanup_data"
    SYNC_HOST2 = "sync_host2"
    ANALYZE_HOST3 = "analyze_host3"
    SYSTEM_HEALTH = "system_health"


class TaskStatus(Enum):
    """–°—Ç–∞—Ç—É—Å—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


@dataclass
class ScheduledTask:
    """–ó–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞"""
    task_type: TaskType
    name: str
    schedule_pattern: str  # "hourly", "daily", "weekly", "0 */2 * * *" (cron-like)
    enabled: bool = True
    last_run: Optional[datetime] = None
    next_run: Optional[datetime] = None
    run_count: int = 0
    failure_count: int = 0
    max_failures: int = 3
    timeout_minutes: int = 60
    params: Dict[str, Any] = field(default_factory=dict)


@dataclass 
class TaskExecution:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏"""
    task_id: str
    task_type: TaskType
    status: TaskStatus
    start_time: datetime
    end_time: Optional[datetime] = None
    duration_seconds: float = 0
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    logs: List[str] = field(default_factory=list)


class SchedulerDaemon:
    """
    –î–µ–º–æ–Ω-–ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á
    
    –†–µ–∞–ª–∏–∑—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    - 2.7. –î–∏—Å–ø–µ—Ç—á–µ—Ä –∑–∞–¥–∞—á
    - 3.2. –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å - –•–æ—Å—Ç 1 (–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö)
    """
    
    def __init__(self, config_path: str = "config/config_v4.json"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞
        
        Args:
            config_path: –ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
        """
        self.config_path = config_path
        self.config = self._load_config()
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (v4)
        # // Chg_V4_DB_2109: –¥–æ–±–∞–≤–ª—è–µ–º v4 –ë–î –∑–∞–¥–∞—á/–≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –≤–µ–±-–ø–∞–Ω–µ–ª–∏ –∏ –∑–∞–≥—Ä—É–∑–æ–∫
        self.db_v4 = TaskDatabase()
        self.dispatcher = TaskDispatcher(config=self.config)
        self.fetcher = VacancyFetcher(
            config=self.config.get('vacancy_fetcher', {}),
            rate_limit_delay=self.config.get('rate_limit_delay', 1.0),
            database=self.db_v4  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ v4 –ë–î
        )
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –¥–µ–º–æ–Ω–∞
        self.running = False
        self.shutdown_requested = False
        
        # –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∏ –∑–∞–¥–∞—á–∏
        self.scheduled_tasks: Dict[str, ScheduledTask] = {}
        self.active_executions: Dict[str, TaskExecution] = {}
        self.execution_history: List[TaskExecution] = []
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        self.check_interval = 60  # –ü—Ä–æ–≤–µ—Ä—è—Ç—å –∫–∞–∂–¥—É—é –º–∏–Ω—É—Ç—É
        self.max_concurrent_tasks = 3
        self.history_limit = 1000
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.logger = logging.getLogger(__name__)
        
        # –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–∏–≥–Ω–∞–ª–æ–≤
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        # –í–µ–±-–ø–∞–Ω–µ–ª—å –ø—Ä–æ—Ü–µ—Å—Å
        self.web_process = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–¥–∞—á –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self._initialize_default_tasks()
    
    def _load_config(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            self.logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é: {e}")
            return {}
    
    def _initialize_default_tasks(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–¥–∞—á –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å–æ–≥–ª–∞—Å–Ω–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º 3.2"""
        
        # 3.2.1. –ó–∞–ø—É—Å–∫ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–∞–∫–∞–Ω—Å–∏–π –∫–∞–∂–¥—ã–π —á–∞—Å
        self.add_task(ScheduledTask(
            task_type=TaskType.FETCH_VACANCIES,
            name="Hourly Vacancy Fetch",
            schedule_pattern="hourly",
            enabled=True,
            timeout_minutes=45,
            params={
                "max_pages": 200,
                "filters_source": "config/filters.json",
                "first_run_delay_sec": 0
            }
        ))
        
        # 3.2.8-3.2.11. –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª–µ–π (–ø–æ—Å–ª–µ –≤–∞–∫–∞–Ω—Å–∏–π)
        self.add_task(ScheduledTask(
            task_type=TaskType.FETCH_EMPLOYERS,
            name="Daily Employer Fetch", 
            schedule_pattern="daily",
            enabled=True,
            timeout_minutes=30,
            params={
                "first_run_delay_sec": 15
            }
        ))
        
        # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∫–∞–∂–¥—ã–µ 6 —á–∞—Å–æ–≤
        self.add_task(ScheduledTask(
            task_type=TaskType.CLEANUP_DATA,
            name="System Cleanup",
            schedule_pattern="0 */6 * * *",  # –ö–∞–∂–¥—ã–µ 6 —á–∞—Å–æ–≤
            enabled=True,
            timeout_minutes=15,
            params={
                "keep_days": 30,
                "vacuum_db": True,
                "first_run_delay_sec": 20
            }
        ))
        
        # // Chg_TASKS_ALWAYS_2009: –í—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á–∏, –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏
        # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Å Host2 (–≤ mock —Ä–µ–∂–∏–º–µ)
        self.add_task(ScheduledTask(
            task_type=TaskType.SYNC_HOST2,
            name="Host2 Sync",
            schedule_pattern="0 */4 * * *",  # –ö–∞–∂–¥—ã–µ 4 —á–∞—Å–∞
            enabled=True,  # –í—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–µ–Ω–æ, mock —Ä–µ–∂–∏–º –±–µ–∑–æ–ø–∞—Å–µ–Ω
            timeout_minutes=20,
            params={
                "first_run_delay_sec": 25
            }
        ))
        
        # –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Host3 (–≤ mock —Ä–µ–∂–∏–º–µ)
        self.add_task(ScheduledTask(
            task_type=TaskType.ANALYZE_HOST3,
            name="Host3 Analysis",
            schedule_pattern="daily",
            enabled=True,  # –í—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–µ–Ω–æ, mock —Ä–µ–∂–∏–º –±–µ–∑–æ–ø–∞—Å–µ–Ω
            timeout_minutes=60,
            params={
                "batch_size": 50,
                "analyze_new_only": True,
                "first_run_delay_sec": 30
            }
        ))
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç
        self.add_task(ScheduledTask(
            task_type=TaskType.SYSTEM_HEALTH,
            name="System Health Check",
            schedule_pattern="*/5 * * * *",  # –ö–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç
            enabled=True,
            timeout_minutes=2,
            params={
                "first_run_delay_sec": 5
            }
        ))
    
    def add_task(self, task: ScheduledTask):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –≤ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫"""
        import uuid
        # –î–æ–±–∞–≤–ª—è–µ–º –º–∏–∫—Ä–æ—Å–µ–∫—É–Ω–¥—ã –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        timestamp = time.time()
        task_id = f"{task.task_type.value}_{int(timestamp)}_{int((timestamp % 1) * 1000000)}_{str(uuid.uuid4())[:8]}"
        self.scheduled_tasks[task_id] = task
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—É—Å–∫
        first_delay = None
        try:
            first_delay = int(task.params.get('first_run_delay_sec')) if task.params else None
        except Exception:
            first_delay = None
        if first_delay and first_delay > 0:
            task.next_run = datetime.now() + timedelta(seconds=first_delay)
        else:
            task.next_run = self._calculate_next_run(task.schedule_pattern)
        
        self.logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–∞ –∑–∞–¥–∞—á–∞: {task.name} (–∑–∞–ø—É—Å–∫: {task.next_run})")
    
    def _calculate_next_run(self, pattern: str) -> datetime:
        """–†–∞—Å—á–µ—Ç –≤—Ä–µ–º–µ–Ω–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—É—Å–∫–∞"""
        now = datetime.now()
        
        if pattern == "hourly":
            return now.replace(minute=0, second=0, microsecond=0) + timedelta(hours=1)
        elif pattern == "daily":
            return now.replace(hour=2, minute=0, second=0, microsecond=0) + timedelta(days=1)
        elif pattern == "weekly":
            days_ahead = 6 - now.weekday()  # –í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ
            if days_ahead <= 0:
                days_ahead += 7
            return now.replace(hour=3, minute=0, second=0, microsecond=0) + timedelta(days=days_ahead)
        elif pattern.startswith("*/"):
            # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è */N –º–∏–Ω—É—Ç
            minutes = int(pattern.split()[0][2:])
            return now + timedelta(minutes=minutes)
        elif pattern.startswith("0 */"):
            # –ü–∞—Ä—Å–∏–Ω–≥ –¥–ª—è 0 */N —á–∞—Å–æ–≤
            hours = int(pattern.split()[1][2:])
            next_hour = (now.hour // hours + 1) * hours
            return now.replace(hour=next_hour % 24, minute=0, second=0, microsecond=0)
        else:
            # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - —á–µ—Ä–µ–∑ —á–∞—Å
            return now + timedelta(hours=1)
    
    async def _execute_task(self, task_id: str, task: ScheduledTask) -> TaskExecution:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏"""
        execution = TaskExecution(
            task_id=task_id,
            task_type=task.task_type,
            status=TaskStatus.RUNNING,
            start_time=datetime.now()
        )
        
        self.active_executions[task_id] = execution
        
        try:
            # // Chg_V4_TASKS_2109: —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –∑–∞–¥–∞—á—É –≤ v4 —Ç–∞–±–ª–∏—Ü–µ tasks –¥–ª—è –≤–µ–±-–ø–∞–Ω–µ–ª–∏
            try:
                v4_type_map = {
                    TaskType.FETCH_VACANCIES: 'load_vacancies',
                    TaskType.FETCH_EMPLOYERS: 'load_vacancies',  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã–π —Ç–∏–ø
                    TaskType.CLEANUP_DATA: 'cleanup',
                    TaskType.SYNC_HOST2: 'process_pipeline',     # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã–π —Ç–∏–ø
                    TaskType.ANALYZE_HOST3: 'process_pipeline',  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã–π —Ç–∏–ø
                    TaskType.SYSTEM_HEALTH: 'test',             # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã–π —Ç–∏–ø
                }
                v4_type = v4_type_map.get(task.task_type, task.task_type.value)
                self.db_v4.create_task(
                    task_id=task_id,
                    task_type=v4_type,
                    params=task.params or {},
                    timeout_sec=int(task.timeout_minutes) * 60
                )
                self.db_v4.update_task_status(task_id, 'running')
            except Exception as reg_err:
                self.logger.error(f"–û—à–∏–±–∫–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ v4-–∑–∞–¥–∞—á–∏ {task_id}: {reg_err}")
            self.logger.info(f"–ù–∞—á–∞–ª–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏: {task.name}")
            execution.logs.append(f"–ó–∞–¥–∞—á–∞ –∑–∞–ø—É—â–µ–Ω–∞: {task.name}")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–¥–∞—á—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
            if task.task_type == TaskType.FETCH_VACANCIES:
                result = await self._execute_fetch_vacancies(task_id, task)
            elif task.task_type == TaskType.FETCH_EMPLOYERS:
                result = await self._execute_fetch_employers(task)
            elif task.task_type == TaskType.CLEANUP_DATA:
                result = await self._execute_cleanup_data(task)
            elif task.task_type == TaskType.SYNC_HOST2:
                result = await self._execute_sync_host2(task)
            elif task.task_type == TaskType.ANALYZE_HOST3:
                result = await self._execute_analyze_host3(task)
            elif task.task_type == TaskType.SYSTEM_HEALTH:
                result = await self._execute_system_health(task)
            else:
                raise Exception(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –∑–∞–¥–∞—á–∏: {task.task_type}")
            
            # –£—Å–ø–µ—à–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
            execution.status = TaskStatus.COMPLETED
            execution.result = result
            execution.logs.append("–ó–∞–¥–∞—á–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            
            task.last_run = execution.start_time
            task.run_count += 1
            task.failure_count = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å v4-–∑–∞–¥–∞—á–∏
            try:
                self.db_v4.update_task_status(task_id, 'completed', result)
            except Exception:
                pass
            self.logger.info(f"–ó–∞–¥–∞—á–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ: {task.name}")
            
        except Exception as e:
            # –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            execution.status = TaskStatus.FAILED
            execution.error = str(e)
            execution.logs.append(f"–û—à–∏–±–∫–∞: {e}")
            
            task.failure_count += 1
            
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ {task.name}: {e}")
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å v4-–∑–∞–¥–∞—á–∏
            try:
                self.db_v4.update_task_status(task_id, 'failed', {'error': str(e)})
            except Exception:
                pass
            
            # –û—Ç–∫–ª—é—á–∞–µ–º –∑–∞–¥–∞—á—É –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–∞ –æ—à–∏–±–æ–∫
            if task.failure_count >= task.max_failures:
                task.enabled = False
                self.logger.warning(f"–ó–∞–¥–∞—á–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∞ –∏–∑-–∑–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫: {task.name}")
        
        finally:
            # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            execution.end_time = datetime.now()
            execution.duration_seconds = (execution.end_time - execution.start_time).total_seconds()
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö –≤ –∏—Å—Ç–æ—Ä–∏—é
            if task_id in self.active_executions:
                del self.active_executions[task_id]
            
            self.execution_history.append(execution)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏
            if len(self.execution_history) > self.history_limit:
                self.execution_history = self.execution_history[-self.history_limit:]
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—É—Å–∫
            if task.enabled:
                task.next_run = self._calculate_next_run(task.schedule_pattern)
            else:
                task.next_run = None
        
        return execution
    
    async def _execute_fetch_vacancies(self, task_id: str, task: ScheduledTask) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–∞–∫–∞–Ω—Å–∏–π (3.2.1 - 3.2.7)"""
        
        # 3.2.2. –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API hh.ru –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ filters.json
        filters_path = task.params.get('filters_source', 'config/filters.json')
        max_pages = task.params.get('max_pages', 200)
        
        try:
            with open(filters_path, 'r', encoding='utf-8') as f:
                raw = json.load(f)
        except Exception as e:
            raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã –∏–∑ {filters_path}: {e}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        stats = {
            'filters_processed': 0,
            'pages_fetched': 0,
            'vacancies_found': 0,
            'vacancies_new': 0,
            'vacancies_duplicates': 0,
            'employers_found': 0,
            'start_time': datetime.now().isoformat()
        }
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ñ–∏–ª—å—Ç—Ä–æ–≤: dict->list, –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–ª—é—á–∞ "filters"
        if isinstance(raw, dict) and 'filters' in raw:
            items = raw['filters']
        elif isinstance(raw, dict):
            items = list(raw.values())
        else:
            items = raw

        active_filters = [flt for flt in items if flt.get('active', flt.get('enabled', True))]

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –∞–∫—Ç–∏–≤–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä
        for flt in active_filters:
            filter_id = flt.get('id', 'unknown')
            filter_name = flt.get('name', filter_id)
            flt_params = flt.get('params', flt)
            self.logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞: {filter_name} ({filter_id})")

            try:
                # 3.2.3. –û—Ü–µ–Ω–∫–∞ —á–∏—Å–ª–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
                try:
                    est_pages = estimate_total_pages(flt_params, self.fetcher)
                except Exception:
                    est_pages = 10
                page_end = max(1, min(int(max_pages), int(est_pages)))

                # 3.2.4-3.2.7. –ü–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ (—á–µ—Ä–µ–∑ v4 –ë–î)
                chunk_result = await asyncio.to_thread(self.fetcher.fetch_chunk, {
                    'page_start': 0,
                    'page_end': page_end,
                    'filter': flt,
                    'task_id': task_id
                })

                stats['filters_processed'] += 1
                stats['pages_fetched'] += int(chunk_result.get('processed_pages', 0))
                loaded = int(chunk_result.get('loaded_count', 0))
                stats['vacancies_found'] += loaded
                stats['vacancies_new'] += loaded

            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∏–ª—å—Ç—Ä–∞ {filter_name}: {e}")
                continue
        
        stats['end_time'] = datetime.now().isoformat()
        stats['duration_minutes'] = (datetime.fromisoformat(stats['end_time']) - 
                                   datetime.fromisoformat(stats['start_time'])).total_seconds() / 60
        
        return stats
    
    async def _execute_fetch_employers(self, task: ScheduledTask) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª–µ–π (3.2.8 - 3.2.11)"""
        
        # 3.2.8. –°–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ ID —Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª–µ–π
        employer_ids = self.db_v4.get_missing_employer_ids()
        
        stats = {
            'employer_ids_found': len(employer_ids),
            'employers_processed': 0,
            'employers_new': 0,
            'errors': 0
        }
        
        # 3.2.10-3.2.11. –ó–∞–ø—Ä–æ—Å –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª–µ–π
        for employer_id in employer_ids[:100]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–∞–∫–µ—Ç
            try:
                # VacancyFetcher.fetch_employer ‚Äî —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è; –≤—ã–ø–æ–ª–Ω—è–µ–º –≤ –ø—É–ª–µ
                employer_data = await asyncio.to_thread(self.fetcher.fetch_employer, employer_id)
                if employer_data:
                    saved_id = self.db_v4.save_employer(employer_data)
                    if saved_id:
                        stats['employers_new'] += 1
                
                stats['employers_processed'] += 1
                
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª—è {employer_id}: {e}")
                stats['errors'] += 1
        
        return stats
    
    async def _execute_cleanup_data(self, task: ScheduledTask) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
        keep_days = task.params.get('keep_days', 30)
        vacuum_db = task.params.get('vacuum_db', True)
        
        stats = {
            'old_records_deleted': 0,
            'temp_files_deleted': 0,
            'database_vacuumed': False
        }
        
        # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π
        cutoff_date = datetime.now() - timedelta(days=keep_days)
        deleted_count = self.db_v4.cleanup_old_records(cutoff_date)
        stats['old_records_deleted'] = deleted_count
        
        # –í–∞–∫—É—É–º –ë–î
        if vacuum_db:
            self.db_v4.vacuum()
            stats['database_vacuumed'] = True
        
        # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        temp_files = list(Path('data').glob('temp_*.sqlite3'))
        for temp_file in temp_files:
            try:
                temp_file.unlink()
                stats['temp_files_deleted'] += 1
            except:
                pass
        
        return stats
    
    async def _execute_sync_host2(self, task: ScheduledTask) -> Dict[str, Any]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Å Host2"""
        if not self.dispatcher.host2_client:
            raise Exception("Host2 client –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        # –ü–æ–ª—É—á–∞–µ–º ID –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
        vacancy_ids = self.db_v4.get_unsynced_vacancy_ids(limit=1000)
        
        # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º
        result = self.dispatcher.sync_to_host2(vacancy_ids)
        # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º —Å—Ç–∞—Ç—É—Å–µ
        try:
            status = (result or {}).get('status', 'ok') if isinstance(result, dict) else 'ok'
            marked = 0
            if status in ('ok', 'success', 'synced'):
                marked = self.db_v4.mark_vacancies_synced(vacancy_ids)
        except Exception:
            marked = 0
        
        return {
            'vacancy_ids_synced': len(vacancy_ids),
            'synced_marked': marked,
            'sync_result': result
        }
    
    async def _execute_analyze_host3(self, task: ScheduledTask) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Host3"""
        if not self.dispatcher.host3_client:
            raise Exception("Host3 client –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        batch_size = task.params.get('batch_size', 50)
        analyze_new_only = task.params.get('analyze_new_only', True)
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        vacancies = self.db_v4.get_unanalyzed_vacancies(
            limit=batch_size, 
            new_only=analyze_new_only
        )
        
        analyzed_count = 0
        for vacancy in vacancies:
            try:
                analysis = self.dispatcher.analyze_with_host3(vacancy)
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
                self.db_v4.save_analysis_result(vacancy['id'], analysis)
                analyzed_count += 1
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∫–∞–Ω—Å–∏–∏ {vacancy['id']}: {e}")
        
        return {
            'vacancies_analyzed': analyzed_count,
            'batch_size': batch_size
        }
    
    async def _execute_system_health(self, task: ScheduledTask) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã"""
        import psutil
        
        # –°–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        health_data = {
            'timestamp': datetime.now().isoformat(),
            'cpu_percent': psutil.cpu_percent(interval=1),
            'memory_percent': psutil.virtual_memory().percent,
            'disk_percent': psutil.disk_usage('/').percent if os.name != 'nt' else psutil.disk_usage('C:\\').percent,
            'database_size_mb': os.path.getsize('data/hh_v4.sqlite3') / (1024*1024) if os.path.exists('data/hh_v4.sqlite3') else 0,
            'active_tasks': len(self.active_executions),
            'host_status': self.dispatcher.get_host_status()
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        alerts = []
        if health_data['cpu_percent'] > 80:
            alerts.append(f"–í—ã—Å–æ–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ CPU: {health_data['cpu_percent']:.1f}%")
        if health_data['memory_percent'] > 85:
            alerts.append(f"–í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {health_data['memory_percent']:.1f}%")
        if health_data['disk_percent'] > 90:
            alerts.append(f"–ó–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –º–µ—Å—Ç–æ –Ω–∞ –¥–∏—Å–∫–µ: {health_data['disk_percent']:.1f}%")
        
        health_data['alerts'] = alerts
        health_data['status'] = 'critical' if alerts else 'healthy'
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
        self.db_v4.save_system_health(health_data)
        
        return health_data
    
    def _signal_handler(self, signum, frame):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è"""
        self.logger.info(f"–ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª {signum}, –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
        self.shutdown_requested = True
    
    def _start_web_panel(self):
        """–ê–≤—Ç–æ–∑–∞–ø—É—Å–∫ –≤–µ–±-–ø–∞–Ω–µ–ª–∏ —Å–æ–≥–ª–∞—Å–Ω–æ 2.4.2 —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∑–∞–Ω—è—Ç–æ–≥–æ –ø–æ—Ä—Ç–∞"""
        import subprocess
        import socket
        
        web_config = self.config.get('web_interface', {})
        if not web_config.get('auto_start', True):
            self.logger.info("–ê–≤—Ç–æ–∑–∞–ø—É—Å–∫ –≤–µ–±-–ø–∞–Ω–µ–ª–∏ –æ—Ç–∫–ª—é—á–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            return
        
        host = web_config.get('host', 'localhost')
        port = int(web_config.get('port', 8000))
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–Ω—è—Ç –ª–∏ –ø–æ—Ä—Ç (–∏, –≤–æ–∑–º–æ–∂–Ω–æ, —Å–µ—Ä–≤–µ—Ä —É–∂–µ –∑–∞–ø—É—â–µ–Ω)
        try:
            with socket.create_connection((host, port), timeout=1):
                self.logger.info(f"üåê –í–µ–±-–ø–∞–Ω–µ–ª—å —É–∂–µ –∞–∫—Ç–∏–≤–Ω–∞ –Ω–∞ http://{host}:{port}/ ‚Äî –∑–∞–ø—É—Å–∫ –ø—Ä–æ–ø—É—â–µ–Ω")
                return
        except Exception:
            pass
        
        try:
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤–µ–±-—Å–µ—Ä–≤–µ—Ä –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å
            # web/server.py —Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ (__main__) –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç host/port
            self.web_process = subprocess.Popen([
                sys.executable, "-m", "web.server"
            ], 
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            cwd=os.getcwd())
            
            # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –≤–µ–±-–ø–∞–Ω–µ–ª—å –≤ –ë–î
            try:
                self.db_v4.register_process(
                    name="web_server", 
                    pid=self.web_process.pid,
                    command_line="web.server",
                    port=port
                )
            except Exception:
                pass
            
            self.logger.info(f"üåê –í–µ–±-–ø–∞–Ω–µ–ª—å –∑–∞–ø—É—â–µ–Ω–∞: http://{host}:{port}/ (PID: {self.web_process.pid})")
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –≤–µ–±-–ø–∞–Ω–µ–ª–∏: {e}")
    
    def _stop_web_panel(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤–µ–±-–ø–∞–Ω–µ–ª–∏"""
        if self.web_process:
            try:
                self.web_process.terminate()
                self.web_process.wait(timeout=10)
                self.logger.info("–í–µ–±-–ø–∞–Ω–µ–ª—å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤–µ–±-–ø–∞–Ω–µ–ª–∏: {e}")
            finally:
                self.web_process = None
    
    async def start(self):
        """–ó–∞–ø—É—Å–∫ –¥–µ–º–æ–Ω–∞"""
        self.logger.info("–ó–∞–ø—É—Å–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ –∑–∞–¥–∞—á HH-–±–æ—Ç–∞ v4")
        self.running = True
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –¥–µ–º–æ–Ω –≤ –ë–î
        self.db_v4.register_process(
            name="scheduler_daemon", 
            pid=os.getpid(),
            command_line="scheduler_daemon.py"
        )
        
        # –ê–≤—Ç–æ—Å—Ç–∞—Ä—Ç –≤–µ–±-–ø–∞–Ω–µ–ª–∏
        self._start_web_panel()
        
        while self.running and not self.shutdown_requested:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
                await self._check_and_execute_tasks()
                
                # –û–∂–∏–¥–∞–µ–º —Å–ª–µ–¥—É—é—â—É—é –ø—Ä–æ–≤–µ—Ä–∫—É
                await asyncio.sleep(self.check_interval)
                
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞: {e}")
                await asyncio.sleep(10)  # –ö–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
        
        # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã
        await self._shutdown()
    
    async def _check_and_execute_tasks(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á"""
        now = datetime.now()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é –∑–∞–¥–∞—á—É
        for task_id, task in list(self.scheduled_tasks.items()):
            if not task.enabled or not task.next_run:
                continue
            
            # –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–∞—Å—Ç—É–ø–∏–ª–æ?
            if now >= task.next_run:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
                if len(self.active_executions) >= self.max_concurrent_tasks:
                    self.logger.warning(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á ({self.max_concurrent_tasks}), –æ—Ç–∫–ª–∞–¥—ã–≤–∞–µ–º {task.name}")
                    continue
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á—É
                asyncio.create_task(self._execute_task(task_id, task))
    
    async def _shutdown(self):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã"""
        self.logger.info("–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞...")
        
        # –û–∂–∏–¥–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á (–º–∞–∫—Å–∏–º—É–º 5 –º–∏–Ω—É—Ç)
        timeout = 300
        start_time = time.time()
        
        while self.active_executions and (time.time() - start_time) < timeout:
            self.logger.info(f"–û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è {len(self.active_executions)} –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á...")
            await asyncio.sleep(5)
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏
        for task_id, execution in self.active_executions.items():
            execution.status = TaskStatus.CANCELLED
            execution.end_time = datetime.now()
            execution.error = "Cancelled due to daemon shutdown"
        
        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤–µ–±-–ø–∞–Ω–µ–ª—å
        self._stop_web_panel()
        
        self.running = False
        self.logger.info("–ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    def get_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞"""
        return {
            'running': self.running,
            'active_tasks': len(self.active_executions),
            'scheduled_tasks': len([t for t in self.scheduled_tasks.values() if t.enabled]),
            'total_executions': len(self.execution_history),
            'last_executions': [
                {
                    'task_type': ex.task_type.value,
                    'status': ex.status.value,
                    'start_time': ex.start_time.isoformat(),
                    'duration': ex.duration_seconds
                }
                for ex in self.execution_history[-10:]
            ]
        }


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è –¥–µ–º–æ–Ω–∞"""
    os.makedirs("logs", exist_ok=True)
    
    # // Chg_UNIFIED_LOG_2009 + Chg_LOG_CFG_2509: —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ ConfigManager
    try:
        cfgm = get_config_manager()
        logging_cfg = cfgm.get_logging_settings()
        log_file = logging_cfg.get('file_path', 'logs/app.log')
        max_bytes = int(logging_cfg.get('max_size_mb', 100)) * 1024 * 1024
        backup_count = int(logging_cfg.get('backup_count', 3))
        level = getattr(logging, str(logging_cfg.get('level', 'INFO')).upper(), logging.INFO)
        console_enabled = bool(logging_cfg.get('console_enabled', True))
        db_enabled = bool(logging_cfg.get('db_enabled', False))

        root = logging.getLogger()
        # –§–∞–π–ª–æ–≤—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ (—Å —Ä–æ—Ç–∞—Ü–∏–µ–π)
        if not any(isinstance(h, RotatingFileHandler) and getattr(h, 'baseFilename', '') == str(Path(log_file)) for h in root.handlers):
            fh = RotatingFileHandler(log_file, maxBytes=max_bytes, backupCount=backup_count, encoding='utf-8')
            fmt = logging.Formatter(logging_cfg.get('format', '%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
            fh.setFormatter(fmt)
            root.addHandler(fh)
        # –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
        if console_enabled and not any(isinstance(h, logging.StreamHandler) for h in root.handlers):
            sh = logging.StreamHandler()
            sh.setFormatter(logging.Formatter(logging_cfg.get('format', '%(asctime)s - %(name)s - %(levelname)s - %(message)s')))
            root.addHandler(sh)
        # –£—Ä–æ–≤–µ–Ω—å
        root.setLevel(level)
    except Exception:
        # –§–æ–ª–±—ç–∫ –Ω–∞ –±–∞–∑–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('logs/app.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
    logger = logging.getLogger(__name__)
    logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ HH-–±–æ—Ç–∞ v4")
    
    try:
        # –°–æ–∑–¥–∞–µ–º PID —Ñ–∞–π–ª
        pid_file = Path("data/scheduler_daemon.pid")
        pid_file.parent.mkdir(exist_ok=True)
        pid_file.write_text(str(os.getpid()))
        logger.info(f"PID —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω: {pid_file} (PID: {os.getpid()})")
        
        daemon = SchedulerDaemon()
        asyncio.run(daemon.start())
    except KeyboardInterrupt:
        logger.info("–ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è")
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞: {e}")
        sys.exit(1)
    finally:
        # –£–¥–∞–ª—è–µ–º PID —Ñ–∞–π–ª –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
        try:
            pid_file = Path("data/scheduler_daemon.pid")
            if pid_file.exists():
                pid_file.unlink()
                logger.info("PID —Ñ–∞–π–ª —É–¥–∞–ª–µ–Ω")
        except:
            pass


if __name__ == "__main__":
    main()
